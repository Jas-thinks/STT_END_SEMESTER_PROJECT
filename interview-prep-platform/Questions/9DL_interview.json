[
  {"id": 1, "question": "Explain how a convolutional layer works and why it is useful for image data."},
  {"id": 2, "question": "What is a receptive field in a CNN and how does it grow through layers?"},
  {"id": 3, "question": "Describe the vanishing gradient problem in deep networks and how ReLU helps alleviate it."},
  {"id": 4, "question": "Compare batch normalization vs layer normalization: when and why use each?"},
  {"id": 5, "question": "Explain residual connections (skip-connections) in deep networks: their motivation and effect on training."},
  {"id": 6, "question": "What is the “double descent” phenomenon in deep learning generalization?"},
  {"id": 7, "question": "Describe how the Adam optimizer works; how does it differ from SGD with momentum?"},
  {"id": 8, "question": "What is a Transformer architecture? Describe its key components (self-attention, feed-forward, positional encoding)."},
  {"id": 9, "question": "How does multi-head self-attention work? Why multiple heads?"},
  {"id": 10, "question": "Why is positional encoding needed in self-attention models? How is it implemented?"},
  {"id": 11, "question": "What is the difference between encoder-only, decoder-only, and encoder-decoder Transformer models?"},
  {"id": 12, "question": "Explain how a Recurrent Neural Network (RNN) works and its limitations."},
  {"id": 13, "question": "What is an LSTM cell? Explain its gates and how it addresses the vanishing gradient in RNNs."},
  {"id": 14, "question": "Compare LSTM vs GRU: what are the differences and trade-offs?"},
  {"id": 15, "question": "What is a Generative Adversarial Network (GAN)? Describe the generator and discriminator training dynamics."},
  {"id": 16, "question": "Explain mode collapse in GANs and techniques used to mitigate it."},
  {"id": 17, "question": "What is a Variational Autoencoder (VAE)? Explain the reparameterization trick."},
  {"id": 18, "question": "How do diffusion models differ from GANs and VAEs in generative modelling?"},
  {"id": 19, "question": "What is self-supervised learning? Give an example of its use in vision or NLP."},
  {"id": 20, "question": "Explain contrastive learning and how it is used to learn embeddings."},
  {"id": 21, "question": "What is knowledge distillation? Explain how it works in deep networks."},
  {"id": 22, "question": "Describe model quantization and pruning: why and how are they applied for deployment?"},
  {"id": 23, "question": "What is mixed precision training? What are its benefits and pitfalls?"},
  {"id": 24, "question": "Explain gradient checkpointing: why is it used and what’s the trade-off?"},
  {"id": 25, "question": "What is the Fisher Information Matrix and how can it relate to neural network pruning or continual learning?"},
  {"id": 26, "question": "Explain catastrophic forgetting in deep learning and approaches to mitigate it."},
  {"id": 27, "question": "What are Mixture-of-Experts (MoE) architectures? How do they scale model capacity efficiently?"},
  {"id": 28, "question": "What is pipeline parallelism vs data parallelism in distributed training large models?"},
  {"id": 29, "question": "Explain how token-sparsity or attention­-sparsity can reduce the computational cost of Transformer models."},
  {"id": 30, "question": "What is retrieval-augmented generation (RAG) and how does it differ from plain language models?"},
  {"id": 31, "question": "What are embeddings? Explain how word embeddings are learned (e.g., Skip-gram, CBOW) and how context matters in modern models."},
  {"id": 32, "question": "Explain the concept of fine-tuning a pretrained model: what layers to freeze/unfreeze and why."},
  {"id": 33, "question": "What are residual networks (ResNet) and how do they enable training much deeper networks?"},
  {"id": 34, "question": "Explain depthwise separable convolutions (as in MobileNet) and why they are efficient."},
  {"id": 35, "question": "What is the lottery ticket hypothesis in deep learning?"},
  {"id": 36, "question": "Describe out-of-distribution (OOD) detection in deep models: why is it challenging?"},
  {"id": 37, "question": "What is adversarial robustness in deep networks? Explain FGSM and how it generates adversarial examples."},
  {"id": 38, "question": "Explain epistemic vs aleatoric uncertainty in deep learning models."},
  {"id": 39, "question": "What is model calibration? How does temperature scaling fix mis-calibrated classifiers?"},
  {"id": 40, "question": "Explain the Hessian of a loss surface for a deep network and how its eigenvalues affect convergence."},
  {"id": 41, "question": "What is the neural tangent kernel (NTK) and how does it explain certain behaviors of wide neural networks?"},
  {"id": 42, "question": "What are scaling laws in deep learning (model size vs data vs compute) and their empirical impact?"},
  {"id": 43, "question": "Explain how sparse attention mechanisms (e.g., Big Bird, Longformer) reduce quadratic complexity."},
  {"id": 44, "question": "What is beam search vs nucleus sampling in sequence generation? When would you use each?"},
  {"id": 45, "question": "Explain RLHF (Reinforcement Learning from Human Feedback) in large language model fine-tuning."},
  {"id": 46, "question": "Describe the architecture of Vision Transformer (ViT) and how it differs from CNNs."},
  {"id": 47, "question": "What is patch embedding in ViT and why is it used?"},
  {"id": 48, "question": "Explain spectral normalization in GANs: its role in stabilizing discriminator training."},
  {"id": 49, "question": "What is classifier-free guidance in diffusion models?"},
  {"id": 50, "question": "Explain how adversarial training improves robustness and what problems it introduces."},
  {"id": 51, "question": "What are the trade-offs between latency, throughput, and accuracy when deploying deep learning models in production at FAANG scale?"},
  {"id": 52, "question": "How do you monitor and handle model drift in production deep learning systems?"}, 
  {"id": 53, "question": "Describe feature stores in ML platforms and their relevance for deep learning pipelines in large organizations."},
  {"id": 54, "question": "Explain how you would design an end-to-end image search system powered by a deep neural network."},
  {"id": 55, "question": "How would you design a recommendation system that uses deep learning embeddings for billions of users and items?"},
  {"id": 56, "question": "What are some challenges of training deep models on multi-modal data (e.g., text + image + video) and how would you address them?"},
  {"id": 57, "question": "Explain the concept of retrieval and ranking in search systems and how deep models are used in ranking features."},
  {"id": 58, "question": "How do you handle cold-start problems in a large scale deep learning recommendation system?"},
  {"id": 59, "question": "What is the role of A/B testing and online experimentation in validating deep learning models at scale?"},
  {"id": 60, "question": "Explain how you would shard and serve a large deep-learning model globally for low latency."},
  {"id": 61, "question": "What is federated learning and how might you design a federated deep learning pipeline in a FAANG context?"},
  {"id": 62, "question": "Describe how you would implement continual learning (incremental updates) for a deployed deep vision model."},
  {"id": 63, "question": "What is the role of feature attribution methods (e.g., SHAP, LIME) in interpreting deep models used in production?"},
  {"id": 64, "question": "How would you architect a deep learning based system for content moderation (image + video) at FAANG scale?"},
  {"id": 65, "question": "Explain how you would estimate and optimize the cost (compute, memory, energy) for training a large-scale DL model."},
  {"id": 66, "question": "What are the pros and cons of model-as-a-service (deep learning API) vs embedded deep inference in devices?"},
  {"id": 67, "question": "How do you ensure fairness, bias mitigation, and ethics in deployed deep models at scale?"},
  {"id": 68, "question": "Explain how knowledge graphs and deep learning can be combined in a large search/recommendation system."},
  {"id": 69, "question": "What are the specific security risks (e.g., data poisoning, model theft) associated with deep learning systems, and how would you guard against them?"},
  {"id": 70, "question": "Describe how you would build a deep-learning pipeline for real-time translation (video+audio) supporting millions of users globally."},
  {"id": 71, "question": "How does gradient clipping help stabilize training of deep networks?"},
  {"id": 72, "question": "Explain the intuition behind LayerNorm in Transformer architectures."},
  {"id": 73, "question": "What are attention masks in Transformers, and why are they necessary?"},
  {"id": 74, "question": "How does positional encoding differ in BERT vs GPT models?"},
  {"id": 75, "question": "What is causal masking and why is it required for autoregressive models?"},
  {"id": 76, "question": "How do you fine-tune a large Transformer model efficiently with limited GPU memory?"},
  {"id": 77, "question": "What is Low-Rank Adaptation (LoRA) and how does it help in model fine-tuning?"},
  {"id": 78, "question": "Explain parameter-efficient fine-tuning (PEFT) methods like adapters and prefix tuning."},
  {"id": 79, "question": "Describe how you would optimize inference speed for a Transformer model in production."},
  {"id": 80, "question": "What are tokenizers, and how do Byte Pair Encoding (BPE) and WordPiece differ?"},
  {"id": 81, "question": "Explain teacher-forcing in RNN training and its pros and cons."},
  {"id": 82, "question": "What are the key differences between CNN-based and Transformer-based vision models?"},
  {"id": 83, "question": "How do you prevent overfitting in deep neural networks apart from dropout and regularization?"},
  {"id": 84, "question": "What are positional embeddings in Vision Transformers and how are they learned?"},
  {"id": 85, "question": "What are diffusion steps in Stable Diffusion models, and how does reverse diffusion work?"},
  {"id": 86, "question": "Explain how CLIP combines image and text embeddings for joint understanding."},
  {"id": 87, "question": "What is the role of contrastive loss in CLIP training?"},
  {"id": 88, "question": "How does cross-attention differ from self-attention in multimodal Transformers?"},
  {"id": 89, "question": "Explain why pretraining on large unlabeled datasets improves downstream performance."},
  {"id": 90, "question": "What is masked language modeling and why was it key to BERT’s success?"},
  {"id": 91, "question": "How does sequence-to-sequence learning differ from encoder-only or decoder-only setups?"},
  {"id": 92, "question": "Explain why positional encodings are crucial in sequence models without recurrence."},
  {"id": 93, "question": "What are hypernetworks and how can they be used in deep learning?"},
  {"id": 94, "question": "What is meta-learning (learning to learn)? Give an example of its use."},
  {"id": 95, "question": "Explain zero-shot vs few-shot learning in large models."},
  {"id": 96, "question": "What are prompt-tuning techniques and why are they effective in large language models?"},
  {"id": 97, "question": "Explain cross-entropy loss in deep classification tasks and how it relates to maximum likelihood estimation."},
  {"id": 98, "question": "What are activation functions and how do you choose between ReLU, GELU, Swish, etc.?"},
  {"id": 99, "question": "Why do deep networks tend to overfit on small datasets? How do you handle that in practice?"},
  {"id": 100, "question": "What is the concept of dropout and why is it effective?"},
  {"id": 101, "question": "What are embeddings used for in recommender systems?"},
  {"id": 102, "question": "Explain cosine similarity and why it’s often used to compare embeddings."},
  {"id": 103, "question": "What is attention pooling and how does it differ from average or max pooling?"},
  {"id": 104, "question": "Explain the difference between hard attention and soft attention."},
  {"id": 105, "question": "How can gradient noise scale with batch size affect model generalization?"},
  {"id": 106, "question": "What is gradient accumulation and how is it useful for training large models?"},
  {"id": 107, "question": "Describe the concept of early stopping and when it might fail."},
  {"id": 108, "question": "Explain how you would design a custom loss function for class imbalance."},
  {"id": 109, "question": "What are embedding bottlenecks and how can you mitigate them?"},
  {"id": 110, "question": "What is label smoothing and why is it beneficial during training?"},
  {"id": 111, "question": "Explain why softmax output probabilities may not be calibrated correctly."},
  {"id": 112, "question": "What is multi-task learning and how can shared representations help performance?"},
  {"id": 113, "question": "Explain transfer learning and why it is central to modern DL applications."},
  {"id": 114, "question": "What is continual pretraining and how is it different from fine-tuning?"},
  {"id": 115, "question": "Explain the difference between deterministic and stochastic depth in ResNets."},
  {"id": 116, "question": "What are graph neural networks (GNNs) and where are they used?"},
  {"id": 117, "question": "How does message passing work in GNNs?"},
  {"id": 118, "question": "What is edge attention and how does it modify standard graph convolution?"},
  {"id": 119, "question": "Explain how temporal graph networks handle dynamic graphs."},
  {"id": 120, "question": "Describe how you would use self-supervised pretraining for a GNN to improve performance on downstream tasks."}
]
